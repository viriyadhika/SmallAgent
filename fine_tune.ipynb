{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3eed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont, Image\n",
    "\n",
    "def pdf_bbox_to_img_bbox(bbox_pdf, scale_x, scale_y):\n",
    "    x0, y0, x1, y1 = bbox_pdf\n",
    "    return (\n",
    "        int(x0 / scale_x),\n",
    "        int(y0 / scale_y),\n",
    "        int(x1 / scale_x),\n",
    "        int(y1 / scale_y),\n",
    "    )\n",
    "\n",
    "def visualize_page(page, viz_items, dpi=300, save_path=None):\n",
    "    # ---- render page ----\n",
    "    pix = page.get_pixmap(dpi=dpi)\n",
    "    img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for item in viz_items:\n",
    "        scale_x = item[\"scale_x\"]\n",
    "        scale_y = item[\"scale_y\"]\n",
    "\n",
    "        # ---- figure bbox ----\n",
    "        fig_bbox = pdf_bbox_to_img_bbox(\n",
    "            item[\"figure_bbox_pdf\"], scale_x, scale_y\n",
    "        )\n",
    "        fig_color = \"red\" if item[\"figure_type\"] == \"Table\" else \"blue\"\n",
    "\n",
    "        draw.rectangle(fig_bbox, outline=fig_color, width=3)\n",
    "        draw.text(\n",
    "            (fig_bbox[0], fig_bbox[1] - 18),\n",
    "            item[\"figure_type\"],\n",
    "            fill=fig_color,\n",
    "            font=font,\n",
    "        )\n",
    "\n",
    "        if item[\"caption_bbox_pdf\"] is not None:\n",
    "            # ---- caption bbox ----\n",
    "            cap_bbox = pdf_bbox_to_img_bbox(\n",
    "                item[\"caption_bbox_pdf\"], scale_x, scale_y\n",
    "            )\n",
    "    \n",
    "            draw.rectangle(cap_bbox, outline=\"green\", width=3)\n",
    "    \n",
    "            caption_preview = item[\"caption_text\"][:60].replace(\"\\n\", \" \")\n",
    "            draw.text(\n",
    "                (cap_bbox[0], cap_bbox[3] + 2),\n",
    "                caption_preview,\n",
    "                fill=\"green\",\n",
    "                font=font,\n",
    "            )\n",
    "\n",
    "    if save_path:\n",
    "        img.save(save_path)\n",
    "\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a271a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to data/models/yolov11x_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x512 3 List-items, 1 Page-header, 1 Picture, 1 Section-header, 8 Texts, 437.0ms\n",
      "Speed: 3.0ms preprocess, 437.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 1: Policy Representations. a) Explicit policy with different types of action representations. b) Implicit policy learns an energy function\n",
      "conditioned on both action and observation and optimizes for actions that minimize the energy landscape c) Diffusion policy refines noise\n",
      "into actions via a learned gradient field. This formulation provides stable training, allows the learned policy to accurately model multimodal\n",
      "action distributions, and accommodates high-dimensional action sequences.\n",
      "CLIP similarity : 0.3609\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "1 Columbia University\n",
      "2 Toyota Research Institute\n",
      "3 MIT\n",
      "https://diffusion-policy.cs.columbia.edu\n",
      "CLIP similarity : 0.2449\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "Cheng Chi1, Siyuan Feng2, Yilun Du3, Zhenjia Xu1, Eric Cousineau2, Benjamin Burchfiel2, Shuran Song1\n",
      "CLIP similarity : 0.1675\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "Diffusion Policy:\n",
      "Visuomotor Policy Learning via Action Diffusion\n",
      "CLIP similarity : 0.2963\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1870, 0.8130]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Formula, 3 List-items, 1 Picture, 2 Section-headers, 14 Texts, 517.5ms\n",
      "Speed: 2.4ms preprocess, 517.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 2: Realworld Benchmarks. We deployed Diffusion Policy on\n",
      "two different robot platforms (UR5 and Franka) for 4 challenging\n",
      "tasks: under-actuate precise pushing (Push-T), 6DoF mug flipping,\n",
      "6DoF sauce pouring, and periodic sauce spreading. Please check out\n",
      "the supplementary material for the resulting robot videos.\n",
      "CLIP similarity : 0.3321\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Starting from xK sampled from Gaussian noise, the DDPM\n",
      "performs K iterations of denoising to produce a series of inter-\n",
      "mediate actions with decreasing levels of noise, xk,xk−1...x0,\n",
      "until a desired noise-free output x0 is formed. The process\n",
      "follows the equation\n",
      "CLIP similarity : 0.2177\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "xk−1 = α(xk −γεθ(xk,k)+N\n",
      "\u0000\n",
      "0,σ2I\n",
      "\u0001\n",
      "),\n",
      "(1)\n",
      "CLIP similarity : 0.1439\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "• Closed-loop action sequences. We combine the policy’s\n",
      "capability to predict high-dimensional action sequences\n",
      "with receding-horizon control to achieve robust execu-\n",
      "tion. This design allows the policy to continuously re-plan\n",
      "its action in a closed-loop manner while maintaining tem-\n",
      "poral action consistency – achieving a balance between\n",
      "long-horizon planning and responsiveness.\n",
      "CLIP similarity : 0.2494\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.2918, 0.7082]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Formula, 1 Picture, 3 Section-headers, 13 Texts, 506.5ms\n",
      "Speed: 3.3ms preprocess, 506.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 3: Diffusion Policy Overview a) General formulation. At time step t, the policy takes the latest To steps of observation data Ot as\n",
      "input and outputs Ta steps of actions At. b) In the CNN-based Diffusion Policy, FiLM (Feature-wise Linear Modulation) [35] conditioning\n",
      "of the observation feature Ot is applied to every convolution layer, channel-wise. Starting from AKt drawn from Gaussian noise, the output\n",
      "of noise-prediction network εθ is subtracted, repeating K times to get A0t , the denoised action sequence. c) In the Transformer-based [52]\n",
      "Diffusion Policy, the embedding of observation Ot is passed into a multi-head cross-attention layer of each transformer decoder block. Each\n",
      "action embedding is constrained to only attend to itself and previous action embeddings (causal attention) using the attention mask illustrated.\n",
      "CLIP similarity : 0.3858\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "make end-to-end training of the vision encoder feasible.\n",
      "Details about the visual encoder are described in Sec. III-B.\n",
      "CLIP similarity : 0.2262\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "C. Diffusion for Visuomotor Policy Learning\n",
      "CLIP similarity : 0.3041\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "III. KEY DESIGN DECISIONS\n",
      "In this section, we describe key design decisions for Diffu-\n",
      "sion Policy as well as its concrete implementation of εθ with\n",
      "neural network architectures.\n",
      "CLIP similarity : 0.3291\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.0621, 0.9379]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Picture, 4 Section-headers, 13 Texts, 541.3ms\n",
      "Speed: 2.6ms preprocess, 541.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 4:\n",
      "Multimodal behavior. At the given state, the end-effector\n",
      "(blue) can either go left or right to push the block. Diffusion Policy\n",
      "learns both modes and commits to only one mode within each rollout.\n",
      "In contrast, both LSTM-GMM [29] and IBC [12] are biased toward\n",
      "one mode, while BET [42] fails to commit to a single mode due to\n",
      "its lack of temporal action consistency. Actions generated by rolling\n",
      "out 40 steps for the best-performing checkpoint.\n",
      "CLIP similarity : 0.3034\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "B. Synergy with Position Control\n",
      "CLIP similarity : 0.2416\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "The challenge of modeling multi-modal distribution in hu-\n",
      "man demonstrations has been widely discussed in behavior\n",
      "cloning literature [12, 42, 29]. Diffusion Policy’s ability to\n",
      "express multimodal distributions naturally and precisely is one\n",
      "of its key advantages.\n",
      "Intuitively, multi-modality in action generation for diffusion\n",
      "policy arises from two sources – an underlying stochastic\n",
      "sampling procedure and a stochastic initialization. In Stochas-\n",
      "tic Langevin Dynamics, an initial sample AK\n",
      "t\n",
      "is drawn from\n",
      "standard Gaussian at the beginning of each sampling process,\n",
      "which helps specify different possible convergence basins for\n",
      "the final action prediction A0\n",
      "t . This action is then further\n",
      "stochastically optimized, with added Gaussian perturbations\n",
      "across a large number of iterations, which enables individual\n",
      "action samples to converge and move between different multi-\n",
      "modal action basins. Fig. 4, shows an example of the Diffusion\n",
      "Policy’s multimodal behavior in a planar pushing task (Push\n",
      "T, introduced below) without explicit demonstration for the\n",
      "tested scenario.\n",
      "CLIP similarity : 0.2481\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "We find that Diffusion Policy with a position-control action\n",
      "space consistently outperforms Diffusion Policy with velocity\n",
      "control, as shown in Fig 5. This surprising result stands in\n",
      "contrast to the majority of recent behavior cloning work that\n",
      "generally relies on velocity control [29, 42, 60, 13, 28, 27].\n",
      "We speculate that there are two primary reasons for this\n",
      "discrepancy: First, action multimodality is more pronounced in\n",
      "position-control mode than it is when using velocity control.\n",
      "Because Diffusion Policy better expresses action multimodal-\n",
      "ity than existing approaches, we speculate that it is inherently\n",
      "less affected by this drawback than existing methods. Further-\n",
      "more, position control suffers less than velocity control from\n",
      "compounding error effects and is thus more suitable for action-\n",
      "sequence prediction (as discussed in the following section). As\n",
      "a result, Diffusion Policy is both less affected by the primary\n",
      "CLIP similarity : 0.2465\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1685, 0.8315]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Formula, 2 List-items, 2 Pictures, 2 Section-headers, 1 Table, 13 Texts, 503.2ms\n",
      "Speed: 2.8ms preprocess, 503.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "drawbacks of position control and is better able to exploit\n",
      "position control’s advantages.\n",
      "CLIP similarity : 0.2775\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Fig. 5: Velocity v.s. Position Control. The performance differ-\n",
      "ence when switching from velocity to position control. While both\n",
      "BCRNN and BET performance decrease, Diffusion Policy is able to\n",
      "leverage the advantage of position and improve its performance.\n",
      "CLIP similarity : 0.3527\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "C. Benefits of Action-Sequence Prediction\n",
      "CLIP similarity : 0.2430\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "-0.6\n",
      "CLIP similarity : 0.1908\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1308, 0.8692]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 6: Diffusion Policy Ablation Study. Change (difference) in\n",
      "success rate relative to the maximum for each task is shown on the Y-\n",
      "axis. Left: trade-off between temporal consistency and responsiveness\n",
      "when selecting the action horizon. Right: Diffusion Policy with\n",
      "position control is robust against latency. Latency is defined as the\n",
      "number of steps between the last frame of observations to the first\n",
      "action that can be executed.\n",
      "CLIP similarity : 0.3202\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "error spikes and unstable evaluation performance throughout\n",
      "the training process, making hyperparameter turning critical\n",
      "and checkpoint selection difficult. As a result, Florence et al.\n",
      "[12] evaluate every checkpoint and report results for the best-\n",
      "performing checkpoint. In a real-world setting, this workflow\n",
      "necessitates the evaluation of many policies on hardware to\n",
      "select a final policy. Here, we discuss why Diffusion Policy\n",
      "appears significantly more stable to train.\n",
      "An implicit policy represents the action distribution using\n",
      "an Energy-Based Model (EBM):\n",
      "CLIP similarity : 0.2887\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "drawbacks of position control and is better able to exploit\n",
      "position control’s advantages.\n",
      "CLIP similarity : 0.2764\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "Fig. 5: Velocity v.s. Position Control. The performance differ-\n",
      "ence when switching from velocity to position control. While both\n",
      "BCRNN and BET performance decrease, Diffusion Policy is able to\n",
      "leverage the advantage of position and improve its performance.\n",
      "CLIP similarity : 0.3102\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.0301, 0.9699]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "| pθ(a|o) | = e−Eθ |\n",
      "| --- | --- |\n",
      "|  | Z(o,θ) | \n",
      "\n",
      "Rank 1 | Score -0.5502\n",
      "e−Eθ (o,a) +∑\n",
      "Nneg\n",
      "j=1e−Eθ (o,ea j) )\n",
      "(7)\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -3.5767\n",
      "LinfoNCE = −log(\n",
      "e−Eθ (o,a)\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -8.0557\n",
      "where Z(o,θ) is an intractable normalization constant (with\n",
      "respect to a).\n",
      "To train the EBM for implicit policy, an InfoNCE-style loss\n",
      "function is used, which equates to the negative log-likelihood\n",
      "of Eq 6:\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.2721, 0.7279]], device='mps:0')\n",
      "\n",
      "0: 640x512 2 Pictures, 1 Section-header, 2 Tables, 8 Texts, 623.2ms\n",
      "Speed: 3.1ms preprocess, 623.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "TABLE II: Behavior Cloning Benchmark (Visual Policy) Performance are reported in the same format as in Tab I. LSTM-GMM numbers\n",
      "were reproduced to get a complete evaluation in addition to the best checkpoint performance reported. Diffusion Policy shows consistent\n",
      "performance improvement, especially for complex tasks like Transport and ToolHang.\n",
      "CLIP similarity : 0.3090\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Fig. 7: Training Stability. Left: IBC fails to infer training actions\n",
      "with increasing accuracy despite smoothly decreasing training loss\n",
      "for energy function. Right: IBC’s evaluation success rate oscillates,\n",
      "making checkpoint selection difficult (evaluated using policy rollouts\n",
      "in simulation).\n",
      "V. EVALUATION\n",
      "We systematically evaluate Diffusion Policy on 12 tasks\n",
      "from 4 benchmarks [12, 15, 29, 42]. This evaluation suite\n",
      "includes both simulated and real environments, single and\n",
      "multiple task benchmarks, fully actuated and under-actuated\n",
      "systems, and rigid and fluid objects. We found Diffusion\n",
      "Policy to consistently outperform the prior state-of-the-art on\n",
      "all of the tested benchmarks, with an average success-rate\n",
      "improvement of 46.9%. In the following sections, we provide\n",
      "an overview of each task, our evaluation methodology on that\n",
      "task, and our key takeaways.\n",
      "CLIP similarity : 0.3321\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "TABLE I: Behavior Cloning Benchmark (State Policy) We present success rates with different checkpoint selection methods in the format\n",
      "of (max performance) / (average of last 10 checkpoints), with each averaged across 3 training seeds and 50 different environment initial\n",
      "conditions (150 in total). LSTM-GMM corresponds to BC-RNN in RoboMimic[29], which we reproduced and obtained slightly better results\n",
      "than the original paper. Our results show that Diffusion Policy significantly improves state-of-the-art performance across the board.\n",
      "CLIP similarity : 0.2550\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "DiffusionPolicy-C\n",
      "1.00/0.98\n",
      "1.00/0.97\n",
      "1.00/0.96\n",
      "1.00/0.96\n",
      "1.00/0.93\n",
      "0.97/0.82\n",
      "0.94/0.82\n",
      "0.68/0.46\n",
      "0.50/0.30\n",
      "0.95/0.91\n",
      "DiffusionPolicy-T\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/0.94\n",
      "1.00/0.89\n",
      "0.95/0.81\n",
      "1.00/0.84\n",
      "0.62/0.35\n",
      "1.00/0.87\n",
      "0.95/0.79\n",
      "CLIP similarity : 0.2791\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.5082, 0.4918]], device='mps:0')\n",
      "tensor([[0.0387, 0.9613]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "|  | Lift ph | mh | Can ph | mh | Square ph | mh | Transport ph | mh | ToolHang ph | Push-T ph |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| LSTM-GMM [29] | 1.00/0.96 | 1.00/0.95 | 1.00/0.88 | 0.98/0.90 | 0.82/0.59 | 0.64/0.38 | 0.88/0.62 | 0.44/0.24 | 0.68/0.49 | 0.69/0.54 |\n",
      "| IBC [12] | 0.94/0.73 | 0.39/0.05 | 0.08/0.01 | 0.00/0.00 | 0.03/0.00 | 0.00/0.00 | 0.00/0.00 | 0.00/0.00 | 0.00/0.00 | 0.75/0.64 |\n",
      "| DiffusionPolicy-C | 1.00/1.00 | 1.00/1.00 | 1.00/0.97 | 1.00/0.96 | 0.98/0.92 | 0.98/0.84 | 1.00/0.93 | 0.89/0.69 | 0.95/0.73 | 0.91/0.84 | \n",
      "\n",
      "Rank 1 | Score -7.7686\n",
      "TABLE II: Behavior Cloning Benchmark (Visual Policy) Performance are reported in the same format as in Tab I. LSTM-GMM numbers\n",
      "were reproduced to get a complete evaluation in addition to the best checkpoint performance reported. Diffusion Policy shows consistent\n",
      "performance improvement, especially for complex tasks like Transport and ToolHang.\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -8.6882\n",
      "DiffusionPolicy-C\n",
      "1.00/0.98\n",
      "1.00/0.97\n",
      "1.00/0.96\n",
      "1.00/0.96\n",
      "1.00/0.93\n",
      "0.97/0.82\n",
      "0.94/0.82\n",
      "0.68/0.46\n",
      "0.50/0.30\n",
      "0.95/0.91\n",
      "DiffusionPolicy-T\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/0.94\n",
      "1.00/0.89\n",
      "0.95/0.81\n",
      "1.00/0.84\n",
      "0.62/0.35\n",
      "1.00/0.87\n",
      "0.95/0.79\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -9.0348\n",
      "TABLE I: Behavior Cloning Benchmark (State Policy) We present success rates with different checkpoint selection methods in the format\n",
      "of (max performance) / (average of last 10 checkpoints), with each averaged across 3 training seeds and 50 different environment initial\n",
      "conditions (150 in total). LSTM-GMM corresponds to BC-RNN in RoboMimic[29], which we reproduced and obtained slightly better results\n",
      "than the original paper. Our results show that Diffusion Policy significantly improves state-of-the-art performance across the board.\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.0387, 0.9613]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "|  | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark | Simulation Benchmark |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Lift | 1 | 1 | 7 | 200 | 300 | 400 | Yes | No |\n",
      "| Can | 1 | 1 | 7 | 200 | 300 | 400 | Yes | No |\n",
      "| Square | 1 | 1 | 7 | 200 | 300 | 400 | Yes | Yes |\n",
      "| Transport | 2 | 3 | 14 | 200 | 300 | 700 | Yes | No |\n",
      "| ToolHang | 1 | 2 | 7 | 200 | 0 | 700 | Yes | Yes |\n",
      "| Push-T | 1 | 1 | 2 | 200 | 0 | 300 | Yes | Yes |\n",
      "| BlockPush | 1 | 2 | 2 | 0 | 0 | 350 | No | No |\n",
      "| Kitchen | 1 | 7 | 9 | 656 | 0 | 280 | No | No |\n",
      "| Realworld Benchmark | Realworld Benchmark | Realworld Benchmark | Rea \n",
      "\n",
      "Rank 1 | Score -6.4910\n",
      "TABLE II: Behavior Cloning Benchmark (Visual Policy) Performance are reported in the same format as in Tab I. LSTM-GMM numbers\n",
      "were reproduced to get a complete evaluation in addition to the best checkpoint performance reported. Diffusion Policy shows consistent\n",
      "performance improvement, especially for complex tasks like Transport and ToolHang.\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -7.1529\n",
      "TABLE III: Tasks Summary. # Rob: number of robots, #Obj: number\n",
      "of objects, ActD: action dimension, PH: proficient-human demonstra-\n",
      "tion, MH: multi-human demonstration, Steps: max number of rollout\n",
      "steps, HiPrec: whether the task has a high precision requirement.\n",
      "BlockPush uses 1000 episodes of scripted demonstrations.\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -7.1814\n",
      "RL. The benchmark consists of 5 tasks with a proficient human\n",
      "(PH) teleoperated demonstration dataset for each and mixed\n",
      "proficient/non-proficient human (MH) demonstration datasets\n",
      "for 4 of the tasks (9 variants in total). For each variant, we\n",
      "report results for both state- and image-based observations.\n",
      "Properties for each task are summarized in Tab III.\n",
      "2) Push-T: adapted from IBC [12], requires pushing a T-\n",
      "shaped block (gray) to a fixed target (red) with a circular\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.0387, 0.9613]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Lift\n",
      "Can\n",
      "Square\n",
      "Transport\n",
      "ToolHang\n",
      "Push-T\n",
      "ph\n",
      "mh\n",
      "ph\n",
      "mh\n",
      "ph\n",
      "mh\n",
      "ph\n",
      "mh\n",
      "ph\n",
      "ph\n",
      "CLIP similarity : 0.2127\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "LSTM-GMM [29]\n",
      "1.00/0.96\n",
      "1.00/0.93\n",
      "1.00/0.91\n",
      "1.00/0.81\n",
      "0.95/0.73\n",
      "0.86/0.59\n",
      "0.76/0.47\n",
      "0.62/0.20\n",
      "0.67/0.31\n",
      "0.67/0.61\n",
      "IBC [12]\n",
      "0.79/0.41\n",
      "0.15/0.02\n",
      "0.00/0.00\n",
      "0.01/0.01\n",
      "0.00/0.00\n",
      "0.00/0.00\n",
      "0.00/0.00\n",
      "0.00/0.00\n",
      "0.00/0.00\n",
      "0.90/0.84\n",
      "BET [42]\n",
      "1.00/0.96\n",
      "1.00/0.99\n",
      "1.00/0.89\n",
      "1.00/0.90\n",
      "0.76/0.52\n",
      "0.68/0.43\n",
      "0.38/0.14\n",
      "0.21/0.06\n",
      "0.58/0.20\n",
      "0.79/0.70\n",
      "CLIP similarity : 0.1790\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "DiffusionPolicy-C\n",
      "1.00/0.98\n",
      "1.00/0.97\n",
      "1.00/0.96\n",
      "1.00/0.96\n",
      "1.00/0.93\n",
      "0.97/0.82\n",
      "0.94/0.82\n",
      "0.68/0.46\n",
      "0.50/0.30\n",
      "0.95/0.91\n",
      "DiffusionPolicy-T\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/1.00\n",
      "1.00/0.94\n",
      "1.00/0.89\n",
      "0.95/0.81\n",
      "1.00/0.84\n",
      "0.62/0.35\n",
      "1.00/0.87\n",
      "0.95/0.79\n",
      "CLIP similarity : 0.2405\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "TABLE I: Behavior Cloning Benchmark (State Policy) We present success rates with different checkpoint selection methods in the format\n",
      "of (max performance) / (average of last 10 checkpoints), with each averaged across 3 training seeds and 50 different environment initial\n",
      "conditions (150 in total). LSTM-GMM corresponds to BC-RNN in RoboMimic[29], which we reproduced and obtained slightly better results\n",
      "than the original paper. Our results show that Diffusion Policy significantly improves state-of-the-art performance across the board.\n",
      "CLIP similarity : 0.1496\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.9850, 0.0150]], device='mps:0')\n",
      "tensor([[0.9025, 0.0975]], device='mps:0')\n",
      "tensor([[0.9958, 0.0042]], device='mps:0')\n",
      "tensor([[0.2020, 0.7980]], device='mps:0')\n",
      "\n",
      "0: 640x512 2 Pictures, 2 Section-headers, 1 Table, 12 Texts, 524.4ms\n",
      "Speed: 2.6ms preprocess, 524.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "C. Key Findings\n",
      "CLIP similarity : 0.2217\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "TABLE IV: Multi-Stage Tasks (State Observation). For PushBlock,\n",
      "px is the frequency of pushing x blocks into the targets. For Kitchen,\n",
      "px is the frequency of interacting with x or more objects (e.g. bottom\n",
      "burner). Diffusion Policy performs better, especially for difficult\n",
      "metrics such as p2 for Block Pushing and p4 for Kitchen, as\n",
      "demonstrated by our results.\n",
      "CLIP similarity : 0.2406\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "seeds and 50 environment initializations 1 (an average of 1500\n",
      "experiments in total). The metric for most tasks is success rate,\n",
      "except for the Push-T task, which uses target area coverage. In\n",
      "addition, we report the average of best-performing checkpoints\n",
      "for robomimic and Push-T tasks to be consistent with the\n",
      "evaluation methodology of their respective original papers\n",
      "[29, 12]. All state-based tasks are trained for 4500 epochs, and\n",
      "image-based tasks for 3000 epochs. Each method is evaluated\n",
      "with its best-performing action space: position control for\n",
      "Diffusion Policy and velocity control for baselines (the effect\n",
      "of action space will be discussed in detail in Sec V-C). The\n",
      "results from these simulation benchmarks are summarized in\n",
      "Table I and Table II.\n",
      "CLIP similarity : 0.2457\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "end-effector (blue)s. Variation is added by random initial\n",
      "conditions for T block and end-effector. The task requires\n",
      "exploiting complex and contact-rich object dynamics to push\n",
      "the T block precisely, using point contacts. There are two\n",
      "variants: one with RGB image observations and another with\n",
      "9 2D keypoints obtained from the ground-truth pose of the T\n",
      "block, both with proprioception for end-effector location.\n",
      "3) Multimodal Block Pushing: adapted from BET [42], this\n",
      "task tests the policy’s ability to model multimodal action dis-\n",
      "tributions by pushing two blocks into two squares in any order.\n",
      "The demonstration data is generated by a scripted oracle with\n",
      "access to groundtruth state info. This oracle randomly selects\n",
      "an initial block to push and moves it to a randomly selected\n",
      "square. The remaining block is then pushed into the remaining\n",
      "square. This task contains long-horizon multimodality that can\n",
      "not be modeled by a single function mapping from observation\n",
      "to action.\n",
      "4) Franka Kitchen: is a popular environment for evaluating\n",
      "the ability of IL and Offline-RL methods to learn multiple\n",
      "long-horizon tasks. Proposed in Relay Policy Learning [15],\n",
      "the Franka Kitchen environment contains 7 objects for inter-\n",
      "action and comes with a human demonstration dataset of 566\n",
      "demonstrations, each completing 4 tasks in arbitrary order. The\n",
      "goal is to execute as many demonstrated tasks as possible,\n",
      "regardless of order, showcasing both short-horizon and long-\n",
      "horizon multimodality.\n",
      "CLIP similarity : 0.2373\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.2089, 0.7911]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "|  | p1 | BlockPush p2 | p1 | p2 | Kitchen p3 | p4 |\n",
      "| --- | --- | --- | --- | --- | --- | --- |\n",
      "| LSTM-GMM [29] | 0.03 | 0.01 | 1.00 | 0.90 | 0.74 | 0.34 |\n",
      "| IBC [12] | 0.01 | 0.00 | 0.99 | 0.87 | 0.61 | 0.24 |\n",
      "| BET [42] | 0.96 | 0.71 | 0.99 | 0.93 | 0.71 | 0.44 |\n",
      "| DiffusionPolicy-C | 0.36 | 0.11 | 1.00 | 1.00 | 1.00 | 0.99 |\n",
      "| DiffusionPolicy-T | 0.99 | 0.94 | 1.00 | 0.99 | 0.99 | 0.96 | \n",
      "\n",
      "Rank 1 | Score -8.3419\n",
      "TABLE IV: Multi-Stage Tasks (State Observation). For PushBlock,\n",
      "px is the frequency of pushing x blocks into the targets. For Kitchen,\n",
      "px is the frequency of interacting with x or more objects (e.g. bottom\n",
      "burner). Diffusion Policy performs better, especially for difficult\n",
      "metrics such as p2 for Block Pushing and p4 for Kitchen, as\n",
      "demonstrated by our results.\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -8.4842\n",
      "end-effector (blue)s. Variation is added by random initial\n",
      "conditions for T block and end-effector. The task requires\n",
      "exploiting complex and contact-rich object dynamics to push\n",
      "the T block precisely, using point contacts. There are two\n",
      "variants: one with RGB image observations and another with\n",
      "9 2D keypoints obtained from the ground-truth pose of the T\n",
      "block, both with proprioception for end-effector location.\n",
      "3) Multimodal Block Pushing: adapted from BET [42], this\n",
      "task tests the policy’s ability to model multimodal action dis-\n",
      "tributions by pushing two blocks into two squares in any order.\n",
      "The demonstration data is generated by a scripted oracle with\n",
      "access to groundtruth state info. This oracle randomly selects\n",
      "an initial block to push and moves it to a randomly selected\n",
      "square. The remaining block is then pushed into the remaining\n",
      "square. This task contains long-horizon multimodality that can\n",
      "not be modeled by a single function mapping from observation\n",
      "to action.\n",
      "4) Franka Kitchen: is a popular environment for evaluating\n",
      "the ability of IL and Offline-RL methods to learn multiple\n",
      "long-horizon tasks. Proposed in Relay Policy Learning [15],\n",
      "the Franka Kitchen environment contains 7 objects for inter-\n",
      "action and comes with a human demonstration dataset of 566\n",
      "demonstrations, each completing 4 tasks in arbitrary order. The\n",
      "goal is to execute as many demonstrated tasks as possible,\n",
      "regardless of order, showcasing both short-horizon and long-\n",
      "horizon multimodality.\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -9.3626\n",
      "seeds and 50 environment initializations 1 (an average of 1500\n",
      "experiments in total). The metric for most tasks is success rate,\n",
      "except for the Push-T task, which uses target area coverage. In\n",
      "addition, we report the average of best-performing checkpoints\n",
      "for robomimic and Push-T tasks to be consistent with the\n",
      "evaluation methodology of their respective original papers\n",
      "[29, 12]. All state-based tasks are trained for 4500 epochs, and\n",
      "image-based tasks for 3000 epochs. Each method is evaluated\n",
      "with its best-performing action space: position control for\n",
      "Diffusion Policy and velocity control for baselines (the effect\n",
      "of action space will be discussed in detail in Sec V-C). The\n",
      "results from these simulation benchmarks are summarized in\n",
      "Table I and Table II.\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.0859, 0.9141]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "TABLE IV: Multi-Stage Tasks (State Observation). For PushBlock,\n",
      "px is the frequency of pushing x blocks into the targets. For Kitchen,\n",
      "px is the frequency of interacting with x or more objects (e.g. bottom\n",
      "burner). Diffusion Policy performs better, especially for difficult\n",
      "metrics such as p2 for Block Pushing and p4 for Kitchen, as\n",
      "demonstrated by our results.\n",
      "CLIP similarity : 0.2542\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "C. Key Findings\n",
      "CLIP similarity : 0.2091\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "seeds and 50 environment initializations 1 (an average of 1500\n",
      "experiments in total). The metric for most tasks is success rate,\n",
      "except for the Push-T task, which uses target area coverage. In\n",
      "addition, we report the average of best-performing checkpoints\n",
      "for robomimic and Push-T tasks to be consistent with the\n",
      "evaluation methodology of their respective original papers\n",
      "[29, 12]. All state-based tasks are trained for 4500 epochs, and\n",
      "image-based tasks for 3000 epochs. Each method is evaluated\n",
      "with its best-performing action space: position control for\n",
      "Diffusion Policy and velocity control for baselines (the effect\n",
      "of action space will be discussed in detail in Sec V-C). The\n",
      "results from these simulation benchmarks are summarized in\n",
      "Table I and Table II.\n",
      "CLIP similarity : 0.2602\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "end-effector (blue)s. Variation is added by random initial\n",
      "conditions for T block and end-effector. The task requires\n",
      "exploiting complex and contact-rich object dynamics to push\n",
      "the T block precisely, using point contacts. There are two\n",
      "variants: one with RGB image observations and another with\n",
      "9 2D keypoints obtained from the ground-truth pose of the T\n",
      "block, both with proprioception for end-effector location.\n",
      "3) Multimodal Block Pushing: adapted from BET [42], this\n",
      "task tests the policy’s ability to model multimodal action dis-\n",
      "tributions by pushing two blocks into two squares in any order.\n",
      "The demonstration data is generated by a scripted oracle with\n",
      "access to groundtruth state info. This oracle randomly selects\n",
      "an initial block to push and moves it to a randomly selected\n",
      "square. The remaining block is then pushed into the remaining\n",
      "square. This task contains long-horizon multimodality that can\n",
      "not be modeled by a single function mapping from observation\n",
      "to action.\n",
      "4) Franka Kitchen: is a popular environment for evaluating\n",
      "the ability of IL and Offline-RL methods to learn multiple\n",
      "long-horizon tasks. Proposed in Relay Policy Learning [15],\n",
      "the Franka Kitchen environment contains 7 objects for inter-\n",
      "action and comes with a human demonstration dataset of 566\n",
      "demonstrations, each completing 4 tasks in arbitrary order. The\n",
      "goal is to execute as many demonstrated tasks as possible,\n",
      "regardless of order, showcasing both short-horizon and long-\n",
      "horizon multimodality.\n",
      "CLIP similarity : 0.2822\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.4759, 0.5241]], device='mps:0')\n",
      "tensor([[0.2089, 0.7911]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Picture, 2 Section-headers, 1 Table, 9 Texts, 519.9ms\n",
      "Speed: 3.0ms preprocess, 519.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "|  | Demo | pos | vel | pos | vel | T-E2E | ImgNet | R3M |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| IoU | 0.84 | 0.14 | 0.19 | 0.24 | 0.25 | 0.53 | 0.24 | 0.66 |\n",
      "| Succ % | 1.00 | 0.00 | 0.00 | 0.20 | 0.10 | 0.65 | 0.15 | 0.80 | \n",
      "\n",
      "Rank 1 | Score -9.5446\n",
      "①\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -9.5446\n",
      "②\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -9.5747\n",
      "TABLE V: Realworld Push-T Experiment. a) Hardware setup. b)\n",
      "Illustration of the task. The robot needs to 1⃝precisely push the T-\n",
      "shaped block into the target region, and 2⃝move the end-effector to\n",
      "the end-zone. c) The ground truth end state used to calculate IoU\n",
      "metrics used in this table. Table: Success is defined by the end-\n",
      "state IoU greater than the minimum IoU in the demonstration dataset.\n",
      "Average episode duration presented in seconds. T-E2E stands for end-\n",
      "to-end trained Transformer-based Diffusion Policy\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.4775, 0.5225]], device='mps:0')\n",
      "tensor([[0.4932, 0.5068]], device='mps:0')\n",
      "tensor([[0.0155, 0.9845]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "TABLE V: Realworld Push-T Experiment. a) Hardware setup. b)\n",
      "Illustration of the task. The robot needs to 1⃝precisely push the T-\n",
      "shaped block into the target region, and 2⃝move the end-effector to\n",
      "the end-zone. c) The ground truth end state used to calculate IoU\n",
      "metrics used in this table. Table: Success is defined by the end-\n",
      "state IoU greater than the minimum IoU in the demonstration dataset.\n",
      "Average episode duration presented in seconds. T-E2E stands for end-\n",
      "to-end trained Transformer-based Diffusion Policy\n",
      "CLIP similarity : 0.2903\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Robustness against latency. Diffusion Policy employs re-\n",
      "ceding horizon position control to predict a sequence of actions\n",
      "into the future. This design helps address the latency gap\n",
      "caused by image processing, policy inference, and network\n",
      "delay. Our ablation study with simulated latency showed\n",
      "Diffusion Policy is able to maintain peak performance with\n",
      "latency up to 4 steps (Fig 6). We also find that velocity control\n",
      "is more affected by latency than position control, likely due\n",
      "to compounding error effects.\n",
      "Diffusion Policy is stable to train. We found that the\n",
      "optimal hyperparameters for Diffusion Policy are mostly con-\n",
      "sistent across tasks. In contrast, IBC [12] is prone to training\n",
      "instability. This property is discussed in Sec IV-D.\n",
      "CLIP similarity : 0.2704\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "task is multi-stage. It requires the robot to 1⃝push the T\n",
      "block into the target and then 2⃝move its end-effector into a\n",
      "designated end-zone to avoid occlusion. 2. The policy needs\n",
      "to make fine adjustments to make sure the T is fully in the\n",
      "goal region before heading to the end-zone, creating additional\n",
      "short-term multimodality. 3. The IoU metric is measured at\n",
      "the last step instead of taking the maximum over all steps.\n",
      "We threshold success rate by the minimum achieved IoU\n",
      "metric from the human demonstration dataset. Our UR5-based\n",
      "experiment setup is shown in Fig V. Diffusion Policy predicts\n",
      "robot commands at 10 Hz and these commands then linearly\n",
      "interpolated to 125 Hz for robot execution.\n",
      "Result Analysis. Diffusion Policy performed close to hu-\n",
      "man level with 95% success rate and 0.8 v.s. 0.84 average\n",
      "IoU, compared with the 0% and 20% success rate of best-\n",
      "performing IBC and LSTM-GMM variants. Fig 8 qualitatively\n",
      "illustrates the behavior for each method starting from the same\n",
      "initial condition. We observed that poor performance during\n",
      "the transition between stages is the most common failure case\n",
      "for the baseline method due to high multimodality during\n",
      "those sections and an ambiguous decision boundary. LSTM-\n",
      "GMM got stuck near the T block in 8 out of 20 evaluations\n",
      "(3rd row), while IBC prematurely left the T block in 6 out\n",
      "of 20 evaluations (4th row). We did not follow the common\n",
      "practice of removing idle actions from training data due to\n",
      "task requirements, which also contributed to LSTM and IBC’s\n",
      "tendency to overfit on small actions and get stuck in this task.\n",
      "The results are best appreciated with videos in supplemental\n",
      "materials.\n",
      "End-to-end v.s. pre-trained vision encoders We tested\n",
      "Diffusion Policy with pre-trained vision encoders (ImageNet\n",
      "[9] and R3M[31]), as seen in Tab. V. Diffusion Policy with\n",
      "R3M achieves an 80% success rate but predicts jittery actions\n",
      "and is more likely to get stuck compared to the end-to-end\n",
      "trained version. Diffusion Policy with ImageNet showed less\n",
      "promising results with abrupt actions and poor performance.\n",
      "We found that end-to-end training is still the most effective\n",
      "way to incorporate visual observation into Diffusion Policy,\n",
      "and our best-performing models were all end-to-end trained.\n",
      "Robustness against perturbation Diffusion Policy’s ro-\n",
      "bustness against visual and physical perturbations was eval-\n",
      "uated in a separate episode from experiments in Tab V. As\n",
      "shown in Fig 9, three types of perturbations are applied. 1)\n",
      "The front camera was blocked for 3 secs by a waving hand\n",
      "(left column), but the diffusion policy, despite exhibiting some\n",
      "jitter, remained on-course and pushed the T block into position.\n",
      "2) We shifted the T block while Diffusion Policy was making\n",
      "fine adjustments to the T block’s position. Diffusion policy\n",
      "immediately re-planned to push from the opposite direction,\n",
      "negating the impact of perturbation. 3) We moved the T\n",
      "block while the robot was en route to the end-zone after\n",
      "the first stage’s completion. The Diffusion Policy immediately\n",
      "changed course to adjust the T block back to its target and\n",
      "then continued to the end-zone. This experiment indicates that\n",
      "Diffusion Policy may be able to synthesize novel behavior\n",
      "in response to unseen observations.\n",
      "CLIP similarity : 0.2884\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "VI. REALWORLD EVALUATION\n",
      "CLIP similarity : 0.2107\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.0155, 0.9845]], device='mps:0')\n",
      "\n",
      "0: 640x512 3 Pictures, 1 Section-header, 6 Texts, 476.9ms\n",
      "Speed: 2.3ms preprocess, 476.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 8: Realworld Push-T Comparisons. Columns 1-4 show action trajectories based on key events. The last column shows averaged images\n",
      "of the end state. A: Diffusion policy (End2End) achieves more accurate and consistent end states. B: Diffusion Policy (R3M) gets stuck\n",
      "initially but later recovers and finishes the task. C: LSTM-GMM fails to reach the end zone while adjusting the T block, blocking the eval\n",
      "camera view. D: IBC prematurely ends the pushing stage.\n",
      "CLIP similarity : 0.2834\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "1\n",
      "CLIP similarity : 0.1847\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "2\n",
      "CLIP similarity : 0.1872\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "Fig. 9: Robustness Test for Diffusion Policy. Left: A waving hand\n",
      "in front of the camera for 3 seconds causes slight jitter, but the\n",
      "predicted actions still function as expected. Middle: Diffusion Policy\n",
      "immediately corrects shifted block position to the goal state during\n",
      "the pushing stage. Right: Policy immediately aborts heading to the\n",
      "end zone, returning the block to goal state upon detecting block shift.\n",
      "This novel behavior was never demonstrated. Please check the videos\n",
      "in the supplementary material.\n",
      "CLIP similarity : 0.3092\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1597, 0.8403]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 9: Robustness Test for Diffusion Policy. Left: A waving hand\n",
      "in front of the camera for 3 seconds causes slight jitter, but the\n",
      "predicted actions still function as expected. Middle: Diffusion Policy\n",
      "immediately corrects shifted block position to the goal state during\n",
      "the pushing stage. Right: Policy immediately aborts heading to the\n",
      "end zone, returning the block to goal state upon detecting block shift.\n",
      "This novel behavior was never demonstrated. Please check the videos\n",
      "in the supplementary material.\n",
      "CLIP similarity : 0.2674\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Fig. 8: Realworld Push-T Comparisons. Columns 1-4 show action trajectories based on key events. The last column shows averaged images\n",
      "of the end state. A: Diffusion policy (End2End) achieves more accurate and consistent end states. B: Diffusion Policy (R3M) gets stuck\n",
      "initially but later recovers and finishes the task. C: LSTM-GMM fails to reach the end zone while adjusting the T block, blocking the eval\n",
      "camera view. D: IBC prematurely ends the pushing stage.\n",
      "CLIP similarity : 0.2274\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "IBC\n",
      "(End2End)\n",
      "CLIP similarity : 0.1648\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "B. Mug Flipping Task\n",
      "CLIP similarity : 0.2002\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1597, 0.8403]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 10: 6DoF Mug Flipping Task. The robot needs to 1⃝Pickup\n",
      "a randomly placed mug and place it lip down (marked orange). 2⃝\n",
      "Rotate the mug such that its handle is pointing left.\n",
      "CLIP similarity : 0.2809\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "D\n",
      "CLIP similarity : 0.1788\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "Fig. 8: Realworld Push-T Comparisons. Columns 1-4 show action trajectories based on key events. The last column shows averaged images\n",
      "of the end state. A: Diffusion policy (End2End) achieves more accurate and consistent end states. B: Diffusion Policy (R3M) gets stuck\n",
      "initially but later recovers and finishes the task. C: LSTM-GMM fails to reach the end zone while adjusting the T block, blocking the eval\n",
      "camera view. D: IBC prematurely ends the pushing stage.\n",
      "CLIP similarity : 0.2173\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "of grasps (forehand vs backhand) or local grasp adjustments\n",
      "(rotation around mug’s principle axis), and are particularly\n",
      "challenging for baseline approaches to capture.\n",
      "Result Analysis. Diffusion policy is able to complete this\n",
      "task with 90% success rate over 20 trials. The richness of\n",
      "captured behaviors is best appreciated with the video. Al-\n",
      "though never demonstrated, the policy is also able to sequence\n",
      "multiple pushes for handle alignment or regrasps for dropped\n",
      "mug when necessary. For comparison, we also train a LSTM-\n",
      "GMM policy trained with a subset of the same data. For 20 in-\n",
      "distribution initial conditions, the LSTM-GMM policy never\n",
      "aligns properly with respect to the mug, and fails to grasp in\n",
      "CLIP similarity : 0.2403\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.0314, 0.9686]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Picture, 3 Section-headers, 1 Table, 10 Texts, 489.2ms\n",
      "Speed: 2.4ms preprocess, 489.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 11: Realworld Sauce Manipulation.\n",
      "[Left] 6DoF pouring\n",
      "Task. The robot needs to 1⃝dip the ladle to scoop sauce from the\n",
      "bowl, 2⃝approach the center of the pizza dough, 3⃝pour sauce, and\n",
      "4⃝lift the ladle to finish the task. [Right] Periodic spreading Task\n",
      "The robot needs to 1⃝approach the center of the sauce with a grasped\n",
      "spoon, 2⃝spread the sauce to cover pizza in a spiral pattern, and 3⃝\n",
      "lift the spoon to finish the task.\n",
      "CLIP similarity : 0.3382\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "all trials.\n",
      "CLIP similarity : 0.1712\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "C. Sauce Pouring and Spreading\n",
      "CLIP similarity : 0.2146\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "are known to be difficult to learn and therefore are often ad-\n",
      "dressed by specialized action representations [58]. Both tasks\n",
      "require the policy to self-terminate by lifting the ladle/spoon.\n",
      "Result Analysis. Diffusion policy achieves close-to-human\n",
      "performance on both tasks, with coverage 0.74 vs 0.79 on\n",
      "pouring and 0.77 vs 0.79 on spreading. Diffusion policy\n",
      "reacted gracefully to external perturbations such as moving the\n",
      "pizza dough by hand during pouring and spreading. Results\n",
      "are best appreciated with videos in the supplemental material.\n",
      "LSTM-GMM performs poorly on both sauce pouring and\n",
      "spreading tasks. It failed to lift the ladle after successfully\n",
      "scooping sauce in 15 out of 20 of the pouring trials. When\n",
      "the ladle was successfully lifted, the sauce was poured off-\n",
      "centered. LSTM-GMM failed to self-terminate in all trials.\n",
      "We suspect LSTM-GMM’s hidden state failed to capture suf-\n",
      "ficiently long history to distinguish between the ladle dipping\n",
      "and the lifting phases of the task. For sauce spreading, LSTM-\n",
      "GMM always lifts the spoon right after the start, and failed to\n",
      "make contact with the sauce in all 20 experiments.\n",
      "CLIP similarity : 0.2528\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.0278, 0.9722]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "|  | IoU | Pour Succ | Spread Coverage | Succ |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Human | 0.79 | 1.00 | 0.79 | 1.00 |\n",
      "| LSTM-GMM Diffusion Policy | 0.06 0.74 | 0.00 0.79 | 0.27 0.77 | 0.00 1.00 | \n",
      "\n",
      "Rank 1 | Score -9.6017\n",
      "Fig. 11: Realworld Sauce Manipulation.\n",
      "[Left] 6DoF pouring\n",
      "Task. The robot needs to 1⃝dip the ladle to scoop sauce from the\n",
      "bowl, 2⃝approach the center of the pizza dough, 3⃝pour sauce, and\n",
      "4⃝lift the ladle to finish the task. [Right] Periodic spreading Task\n",
      "The robot needs to 1⃝approach the center of the sauce with a grasped\n",
      "spoon, 2⃝spread the sauce to cover pizza in a spiral pattern, and 3⃝\n",
      "lift the spoon to finish the task.\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -10.0026\n",
      "3\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -10.0222\n",
      "2\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.0278, 0.9722]], device='mps:0')\n",
      "\n",
      "0: 640x512 6 List-items, 4 Section-headers, 9 Texts, 519.3ms\n",
      "Speed: 3.2ms preprocess, 519.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x512 26 List-items, 1 Section-header, 1 Text, 494.7ms\n",
      "Speed: 2.7ms preprocess, 494.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x512 25 List-items, 1 Text, 510.9ms\n",
      "Speed: 2.6ms preprocess, 510.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x512 2 List-items, 2 Pictures, 5 Section-headers, 13 Texts, 493.3ms\n",
      "Speed: 2.5ms preprocess, 493.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 12: Observation Horizon Ablation Study. State-based Dif-\n",
      "fusion Policy is not sensitive to observation horizon. Vision-based\n",
      "Diffusion Policy prefers low but > 1 observation horizon, with 2\n",
      "being a good compromise for most tasks.\n",
      "CLIP similarity : 0.3133\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Fig. 13: Data Efficiency Ablation Study. Diffusion Policy outper-\n",
      "forms LSTM-GMM [29] at every training dataset size.\n",
      "CLIP similarity : 0.2874\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "Conference on Robotics and Automation (ICRA), pages\n",
      "8658–8665. IEEE, 2022. 10\n",
      "[59] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan\n",
      "Welker, Jonathan Chien, Maria Attarian, Travis Arm-\n",
      "strong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al.\n",
      "Transporter networks: Rearranging the visual world for\n",
      "robotic manipulation. In Conference on Robot Learning,\n",
      "pages 726–747. PMLR, 2021. 10\n",
      "[60] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee,\n",
      "Xi Chen, Ken Goldberg, and Pieter Abbeel.\n",
      "Deep\n",
      "imitation learning for complex manipulation tasks from\n",
      "virtual reality teleoperation. In 2018 IEEE International\n",
      "Conference on Robotics and Automation (ICRA), pages\n",
      "5628–5635. IEEE, 2018. 4, 7, 10\n",
      "[61] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and\n",
      "Hao Li.\n",
      "On the continuity of rotation representations\n",
      "in neural networks.\n",
      "In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition,\n",
      "pages 5745–5753, 2019. 14\n",
      "CLIP similarity : 0.2561\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "APPENDIX\n",
      "CLIP similarity : 0.2087\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1555, 0.8445]], device='mps:0')\n",
      "\n",
      "=============== FIGURE CAPTION SCORING (CLIP ONLY) ===============\n",
      "Candidate 1\n",
      "Text:\n",
      "Fig. 13: Data Efficiency Ablation Study. Diffusion Policy outper-\n",
      "forms LSTM-GMM [29] at every training dataset size.\n",
      "CLIP similarity : 0.3548\n",
      "------------------------------------------------------------\n",
      "Candidate 2\n",
      "Text:\n",
      "Fig. 12: Observation Horizon Ablation Study. State-based Dif-\n",
      "fusion Policy is not sensitive to observation horizon. Vision-based\n",
      "Diffusion Policy prefers low but > 1 observation horizon, with 2\n",
      "being a good compromise for most tasks.\n",
      "CLIP similarity : 0.2649\n",
      "------------------------------------------------------------\n",
      "Candidate 3\n",
      "Text:\n",
      "0.2\n",
      "CLIP similarity : 0.2068\n",
      "------------------------------------------------------------\n",
      "Candidate 4\n",
      "Text:\n",
      "0.1\n",
      "CLIP similarity : 0.2049\n",
      "------------------------------------------------------------\n",
      "=================================================================\n",
      "\n",
      "tensor([[0.1083, 0.8917]], device='mps:0')\n",
      "\n",
      "0: 640x512 5 Section-headers, 2 Tables, 12 Texts, 469.4ms\n",
      "Speed: 2.3ms preprocess, 469.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "| Lift | Pos | 2 | 8 | 10 | 9 | 22 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Can | Pos | 2 | 8 | 10 | 9 | 22 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| Square | Pos | 2 | 8 | 10 | 9 | 22 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| Transport | Pos | 2 | 8 | 10 | 9 | 45 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| ToolHang | Pos | 2 | 8 | 10 | 9 | 22 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| Push-T | Pos | 2 | 8 | 16 | 9 | 22 | 8 | 256 | 0.01 | 1e-4 | 1e-1 | 100 | 100 |\n",
      "| Block Push | Vel | 3 | 1 | 5 | 9 | 0 | 8 | 256 | 0.3 | 1e-4 | 1e-3 | 100 | 100 |\n",
      "| Kitchen | Pos | 4 | 8 | 16 | 80 | 0 | 8 | 768 | 0.1 | 1e-4 | 1e-3 | 10 \n",
      "\n",
      "Rank 1 | Score -9.2736\n",
      "TABLE VII: Hyperparameters for Transformer-based Diffusion Policy Ctrl: position or velocity control To: observation horizon Ta: action\n",
      "horizon Tp: action prediction horizon #D-Params: diffusion network number of parameters in millions #V-Params: vision encoder number\n",
      "of parameters in millions Emb Dim: transformer token embedding dimension Attn Dropout: transformer attention dropout probability Lr:\n",
      "learining rate WDecay: weight decay (for transformer only) D-Iters Train: number of training diffusion iterations D-Iters Eval: number of\n",
      "inference diffusion iterations (enabled by DDIM [45])\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -9.3020\n",
      "TABLE VI: Hyperparameters for CNN-based Diffusion Policy Ctrl: position or velocity control To: observation horizon Ta: action horizon\n",
      "Tp: action prediction horizon ImgRes: environment observation resolution (Camera views x W x H) CropRes: random crop resolution #D-\n",
      "Params: diffusion network number of parameters in millions #V-Params: vision encoder number of parameters in millions Lr: learining rate\n",
      "WDecay: weight decay D-Iters Train: number of training diffusion iterations D-Iters Eval: number of inference diffusion iterations (enabled\n",
      "by DDIM [45])\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -9.9565\n",
      "F. Observation Horizon\n",
      "We found state-based Diffusion Policy to be insensitive to\n",
      "observation horizon, as shown in Fig. 12. However, vision-\n",
      "based Diffusion Policy, in particular the variant with CNN\n",
      "backbone, see performance decrease with increasing observa-\n",
      "tion horizon. In practice, we found an observation horizon\n",
      "of 2 is good for most of the tasks for both state and image\n",
      "observations.\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.4078, 0.5922]], device='mps:0')\n",
      "tensor([[0.2202, 0.7798]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 89.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== TABLE CAPTION (MARKDOWN QUERY) ===========\n",
      "QUERY:\n",
      "Which sentence is the title or caption describing the following table?\n",
      "\n",
      "| Lift | Pos | 2 | 8 | 16 | 2x84x84 | 2x76x76 | 256 | 22 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Can | Pos | 2 | 8 | 16 | 2x84x84 | 2x76x76 | 256 | 22 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| Square | Pos | 2 | 8 | 16 | 2x84x84 | 2x76x76 | 256 | 22 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| Transport | Pos | 2 | 8 | 16 | 4x84x85 | 4x76x76 | 264 | 45 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| ToolHang | Pos | 2 | 8 | 16 | 2x240x240 | 2x216x216 | 256 | 22 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| Push-T | Pos | 2 | 8 | 16 | 1x96x96 | 1x84x84 | 256 | 22 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| Block Push | Pos | 3 | 1 | 12 | N/A | N/A | 256 | 0 | 1e-4 | 1e-6 | 100 | 100 |\n",
      "| Kitchen | Pos | 2 | 8 | 16 | N/A | N/A \n",
      "\n",
      "Rank 1 | Score -9.0828\n",
      "TABLE VI: Hyperparameters for CNN-based Diffusion Policy Ctrl: position or velocity control To: observation horizon Ta: action horizon\n",
      "Tp: action prediction horizon ImgRes: environment observation resolution (Camera views x W x H) CropRes: random crop resolution #D-\n",
      "Params: diffusion network number of parameters in millions #V-Params: vision encoder number of parameters in millions Lr: learining rate\n",
      "WDecay: weight decay D-Iters Train: number of training diffusion iterations D-Iters Eval: number of inference diffusion iterations (enabled\n",
      "by DDIM [45])\n",
      "------------------------------------------------------------\n",
      "Rank 2 | Score -9.0843\n",
      "TABLE VII: Hyperparameters for Transformer-based Diffusion Policy Ctrl: position or velocity control To: observation horizon Ta: action\n",
      "horizon Tp: action prediction horizon #D-Params: diffusion network number of parameters in millions #V-Params: vision encoder number\n",
      "of parameters in millions Emb Dim: transformer token embedding dimension Attn Dropout: transformer attention dropout probability Lr:\n",
      "learining rate WDecay: weight decay (for transformer only) D-Iters Train: number of training diffusion iterations D-Iters Eval: number of\n",
      "inference diffusion iterations (enabled by DDIM [45])\n",
      "------------------------------------------------------------\n",
      "Rank 3 | Score -9.3035\n",
      "E. Data Efficiency\n",
      "We found Diffusion Policy to outperform LSTM-GMM [29]\n",
      "at every training dataset size, as shown in Fig. 13.\n",
      "------------------------------------------------------------\n",
      "=====================================================\n",
      "\n",
      "tensor([[0.2202, 0.7798]], device='mps:0')\n",
      "\n",
      "0: 640x512 1 Section-header, 13 Texts, 465.1ms\n",
      "Speed: 2.3ms preprocess, 465.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    }
   ],
   "source": [
    "from app.pdf_agent import PDFTableExtractor\n",
    "import fitz\n",
    "from pathlib import Path\n",
    "\n",
    "extractor = PDFTableExtractor(device=\"mps\")\n",
    "\n",
    "doc = fitz.open(\"data/downloads/manual_down/diffusion_policy.pdf\")\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    outs, viz_items = extractor.extract(page)\n",
    "    if not viz_items:\n",
    "        continue\n",
    "\n",
    "    img = visualize_page(\n",
    "        page,\n",
    "        viz_items,\n",
    "        dpi=extractor.dpi,\n",
    "        save_path=f\"data/extracted_tables/page_{i:03d}_viz.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1032dafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=2)\n",
    "# model.classifier.load_state_dict(torch.load(\"data/roberta_figure_diff.pt\"))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148c4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1113ec090> <torch.utils.data.dataloader.DataLoader object at 0x358f61990>\n",
      "('REFERENCES', 'The following table presents research and development expenditures.', 'The table below shows cash and cash equivalents at period end.', 'Tourists gather near the Colosseum during peak summer travel season.', 'He continues to work part-time as an anesthesiologist while overseeing daily operations.', '15794', 'The clinic’s expansion followed months of regulatory review and internal planning. Management opted to focus on a narrow range of procedures to maximize efficiency. The model has since drawn interest from other practitioners considering similar ventures.', 'data not shown', 'Emissions By Industry', 'We have entered into operating and finance lease agreement primarily for data centers, land, and offices throughout the World', 'Figure VIII. Year-over-year growth of subscription revenue from 2018 to 2025.', 'The company’s decision to delay its product launch was influenced by supply chain disruptions and rising input costs. Executives emphasized the importance of maintaining quality standards rather than rushing to market. Investors responded cautiously to the announcement.', 'While he said the immediate emergency has been stabilized, the city is “still very much in the red zone,” with an “incredible” amount of work still to be done.', 'The lack of rebound in oil and gas investment reflects ongoing uncertainty about future demand.', 'Table 4. Comparison of the calculated band gap (Eg) values for the cubic and hexagonal phases of AlSb using the HSE06 and mBJ methods. ', 'FIG 14. Rollout visualization comparing expert trajectories and NWM-ranked trajectories. Higher-ranked trajectories remain closer to the ground truth.', 'SMALL BUSINESS EMPLOYMENT SHARE', 'Central banks have signaled caution in adjusting monetary policy.', 'et al.', 'However, households are becoming more cautious as borrowing costs increase.', 'EMPLOYMENT GROWTH RATE', 'The presence of a direct band gap promotes efficient generation and recombination of electron- hole pairs in the studied material. This property enhances both light absorption and  photoluminescence, which are critical for the performance of optoelectronic devices such as  light-emitting diodes (LEDs) and lasers. Moreover, materials with a wide and direct band gap  can  reduce  energy  losses  caused  by  thermal  conductivity  and  enable  stable  operation  of ', 'Health-care providers are increasingly adopting outpatient treatment models to reduce hospital congestion.', 'TABLE 6. Comparison of computational cost and prediction accuracy across model sizes. FLOPs are measured per generated frame.', 'The largest source of cash are advertising revenues and interest payments. We also generate cash through subscription and miscellaneous fees', 'CLIMATE ADAPTATION PROJECTS', 'After several rounds of negotiations, the company secured private financing to fund its expansion plans. The new capital allowed management to proceed with opening additional locations while maintaining existing operations. Executives say the deal provides stability in an otherwise volatile market.', 'Public infrastructure projects have been delayed by rising construction costs and labor shortages. Municipal governments are reassessing timelines and budgets as a result. In some cases, projects have been scaled back to remain financially viable.', 'Training Objective. The model is trained to minimize the mean-squared between the clean and predicted target, aim- ing to learn the denoising process:', 'Rural communities continue to face challenges attracting and retaining skilled professionals. Limited infrastructure and fewer employment opportunities for spouses are often cited as key barriers. Some regions are experimenting with incentive programs to address the issue.', 'Despite advances in medical imaging and minimally invasive treatments, access to specialized care remains uneven. Patients in smaller communities often travel long distances for procedures that are readily available in major cities. Policymakers have identified this disparity as a growing concern.', 'TRADE BALANCE OVER TIME') tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class ClassifDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(\"dataset_example.csv\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.loc[index][\"text\"], self.data.loc[index][\"label\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "dataset = ClassifDataset()\n",
    "train_data, test_data = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train, test)\n",
    "\n",
    "for X, y in train:\n",
    "    print(X, y)\n",
    "    break\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063f9493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.7019778192043304 test loss = 0.6649491190910339 accuracy = 0.5476190447807312\n",
      "loss = 0.6758884092171987 test loss = 0.6468484997749329 accuracy = 0.6904761791229248\n",
      "loss = 0.6820089817047119 test loss = 0.6601446866989136 accuracy = 0.5476190447807312\n",
      "loss = 0.6638355751832327 test loss = 0.6160564124584198 accuracy = 0.8095238208770752\n",
      "loss = 0.6088594496250153 test loss = 0.6102079451084137 accuracy = 0.7142857313156128\n",
      "loss = 0.6474902828534445 test loss = 0.601288765668869 accuracy = 0.738095223903656\n",
      "loss = 0.6061618129412333 test loss = 0.5861169993877411 accuracy = 0.8333333134651184\n",
      "loss = 0.5856964687506357 test loss = 0.5737299025058746 accuracy = 0.8333333134651184\n",
      "loss = 0.5617215236028036 test loss = 0.577080488204956 accuracy = 0.8095238208770752\n",
      "loss = 0.5263089587291082 test loss = 0.551364004611969 accuracy = 0.8333333134651184\n",
      "loss = 0.5471353083848953 test loss = 0.5369485020637512 accuracy = 0.8333333134651184\n",
      "loss = 0.5189899106820425 test loss = 0.5245751291513443 accuracy = 0.8333333134651184\n",
      "loss = 0.4755910237630208 test loss = 0.5224443078041077 accuracy = 0.8333333134651184\n",
      "loss = 0.4677430788675944 test loss = 0.5167868882417679 accuracy = 0.8095238208770752\n",
      "loss = 0.48728208243846893 test loss = 0.4938593953847885 accuracy = 0.8333333134651184\n",
      "loss = 0.4405866265296936 test loss = 0.48228706419467926 accuracy = 0.8333333134651184\n",
      "loss = 0.4580706059932709 test loss = 0.4787036329507828 accuracy = 0.8333333134651184\n",
      "loss = 0.4572550853093465 test loss = 0.47577981650829315 accuracy = 0.8333333134651184\n",
      "loss = 0.404253492752711 test loss = 0.45533162355422974 accuracy = 0.8333333134651184\n",
      "loss = 0.40724937121073407 test loss = 0.4539899528026581 accuracy = 0.8571428656578064\n",
      "loss = 0.37598032255967456 test loss = 0.4361551105976105 accuracy = 0.8333333134651184\n",
      "loss = 0.40935783088207245 test loss = 0.4304019957780838 accuracy = 0.8333333134651184\n",
      "loss = 0.3637371410926183 test loss = 0.42846347391605377 accuracy = 0.8333333134651184\n",
      "loss = 0.3487403492132823 test loss = 0.4153361916542053 accuracy = 0.8571428656578064\n",
      "loss = 0.3973456819852193 test loss = 0.4161221534013748 accuracy = 0.8809523582458496\n",
      "loss = 0.33315786222616833 test loss = 0.4127352684736252 accuracy = 0.8571428656578064\n",
      "loss = 0.3303259511788686 test loss = 0.39643362164497375 accuracy = 0.8571428656578064\n",
      "loss = 0.35670176645119983 test loss = 0.38944272696971893 accuracy = 0.8571428656578064\n",
      "loss = 0.35744809607664746 test loss = 0.37804490327835083 accuracy = 0.8571428656578064\n",
      "loss = 0.3272743324438731 test loss = 0.37265370786190033 accuracy = 0.8571428656578064\n",
      "loss = 0.29458709557851154 test loss = 0.3867368847131729 accuracy = 0.8809523582458496\n",
      "loss = 0.3014100765188535 test loss = 0.3677987605333328 accuracy = 0.8809523582458496\n",
      "loss = 0.3529077097773552 test loss = 0.3794380724430084 accuracy = 0.8571428656578064\n",
      "loss = 0.36757857104142505 test loss = 0.4082190543413162 accuracy = 0.8333333134651184\n",
      "loss = 0.37489831695954007 test loss = 0.4749591797590256 accuracy = 0.8095238208770752\n",
      "loss = 0.38664647936820984 test loss = 0.3666989356279373 accuracy = 0.8809523582458496\n",
      "loss = 0.33814749866724014 test loss = 0.36365438997745514 accuracy = 0.8809523582458496\n",
      "loss = 0.3631057118376096 test loss = 0.3551858514547348 accuracy = 0.8809523582458496\n",
      "loss = 0.3331436862548192 test loss = 0.3701304644346237 accuracy = 0.8571428656578064\n",
      "loss = 0.29844652613004047 test loss = 0.3336165100336075 accuracy = 0.8809523582458496\n",
      "loss = 0.3548297385374705 test loss = 0.3515065163373947 accuracy = 0.8809523582458496\n",
      "loss = 0.28871896862983704 test loss = 0.3376021385192871 accuracy = 0.8571428656578064\n",
      "loss = 0.27216506004333496 test loss = 0.35767532885074615 accuracy = 0.8571428656578064\n",
      "loss = 0.29454346001148224 test loss = 0.3320600539445877 accuracy = 0.8809523582458496\n",
      "loss = 0.2755366613467534 test loss = 0.33697137236595154 accuracy = 0.9047619104385376\n",
      "loss = 0.27895916253328323 test loss = 0.32834480702877045 accuracy = 0.8809523582458496\n",
      "loss = 0.3016277427474658 test loss = 0.30291956663131714 accuracy = 0.8809523582458496\n",
      "loss = 0.33017602066198987 test loss = 0.3049211651086807 accuracy = 0.8809523582458496\n",
      "loss = 0.2837669129172961 test loss = 0.2927265465259552 accuracy = 0.8809523582458496\n",
      "loss = 0.22603809585173926 test loss = 0.31103406101465225 accuracy = 0.8571428656578064\n",
      "loss = 0.29068606843551 test loss = 0.29092173278331757 accuracy = 0.8809523582458496\n",
      "loss = 0.2778400667011738 test loss = 0.2997900992631912 accuracy = 0.9285714030265808\n",
      "loss = 0.292101410528024 test loss = 0.3323502242565155 accuracy = 0.8571428656578064\n",
      "loss = 0.24122046679258347 test loss = 0.2845482900738716 accuracy = 0.8809523582458496\n",
      "loss = 0.26029565433661145 test loss = 0.3159208744764328 accuracy = 0.8809523582458496\n",
      "loss = 0.2628161981701851 test loss = 0.2865006849169731 accuracy = 0.9285714030265808\n",
      "loss = 0.2185236488779386 test loss = 0.29623404145240784 accuracy = 0.9047619104385376\n",
      "loss = 0.22455088794231415 test loss = 0.2873137891292572 accuracy = 0.9285714030265808\n",
      "loss = 0.2142424906293551 test loss = 0.2771255820989609 accuracy = 0.9285714030265808\n",
      "loss = 0.2045801803469658 test loss = 0.2903404161334038 accuracy = 0.8809523582458496\n",
      "loss = 0.22328729182481766 test loss = 0.2655170112848282 accuracy = 0.8809523582458496\n",
      "loss = 0.22313754012187323 test loss = 0.2616189792752266 accuracy = 0.8809523582458496\n",
      "loss = 0.22815372546513876 test loss = 0.2600290775299072 accuracy = 0.9047619104385376\n",
      "loss = 0.22522680958112082 test loss = 0.25775814056396484 accuracy = 0.9047619104385376\n",
      "loss = 0.21772835403680801 test loss = 0.263705737888813 accuracy = 0.9285714030265808\n",
      "loss = 0.22232011705636978 test loss = 0.2622825428843498 accuracy = 0.9285714030265808\n",
      "loss = 0.20535070697466531 test loss = 0.2511739358305931 accuracy = 0.8809523582458496\n",
      "loss = 0.19850256542364755 test loss = 0.25779541581869125 accuracy = 0.9285714030265808\n",
      "loss = 0.2577851166327794 test loss = 0.2679795026779175 accuracy = 0.9285714030265808\n",
      "loss = 0.19400117670496306 test loss = 0.24887323379516602 accuracy = 0.9047619104385376\n",
      "loss = 0.25028116162866354 test loss = 0.3718408942222595 accuracy = 0.8333333134651184\n",
      "loss = 0.2947146122654279 test loss = 0.23963893204927444 accuracy = 0.9285714030265808\n",
      "loss = 0.26569613566001254 test loss = 0.2548390030860901 accuracy = 0.9047619104385376\n",
      "loss = 0.18996777882178625 test loss = 0.26635584235191345 accuracy = 0.9047619104385376\n",
      "loss = 0.16470223665237427 test loss = 0.2676447704434395 accuracy = 0.9285714030265808\n",
      "loss = 0.2249228556950887 test loss = 0.25385959446430206 accuracy = 0.9047619104385376\n",
      "loss = 0.17106490271786848 test loss = 0.2621512860059738 accuracy = 0.9285714030265808\n",
      "loss = 0.1973392330110073 test loss = 0.25293730199337006 accuracy = 0.9285714030265808\n",
      "loss = 0.20630263164639473 test loss = 0.2388179823756218 accuracy = 0.9285714030265808\n",
      "loss = 0.19961228718360266 test loss = 0.24402696639299393 accuracy = 0.9285714030265808\n",
      "loss = 0.19401384765903154 test loss = 0.27460167557001114 accuracy = 0.9047619104385376\n",
      "loss = 0.21536844658354917 test loss = 0.23411405831575394 accuracy = 0.9285714030265808\n",
      "loss = 0.22022578368584314 test loss = 0.2330322191119194 accuracy = 0.9047619104385376\n",
      "loss = 0.1958807185292244 test loss = 0.2377488687634468 accuracy = 0.9285714030265808\n",
      "loss = 0.20015517808496952 test loss = 0.28379736095666885 accuracy = 0.8809523582458496\n",
      "loss = 0.2251033658782641 test loss = 0.24985653162002563 accuracy = 0.9285714030265808\n",
      "loss = 0.1918934310475985 test loss = 0.2178076207637787 accuracy = 0.9047619104385376\n",
      "loss = 0.19564168093105158 test loss = 0.26005564630031586 accuracy = 0.9047619104385376\n",
      "loss = 0.21177691469589868 test loss = 0.21183709800243378 accuracy = 0.9285714030265808\n",
      "loss = 0.27646008506417274 test loss = 0.22889750450849533 accuracy = 0.9285714030265808\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "epochs = 90\n",
    "\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_test_loss = 0\n",
    "    for X, y in train:\n",
    "        tokens = tokenizer(X, return_tensors=\"pt\", padding=True).to(device)\n",
    "        y = y.to(device)\n",
    "        ypred = model(**tokens).logits\n",
    "        \n",
    "        loss = ce(ypred, y)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_dt = 0\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test:\n",
    "            tokens = tokenizer(X_test, return_tensors=\"pt\", padding=True).to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            ypred_test = model(**tokens).logits\n",
    "\n",
    "            loss = ce(ypred_test, y_test)\n",
    "            epoch_test_loss += loss.item()\n",
    "            correct += torch.sum((ypred_test.argmax(dim=1) == y_test).int())\n",
    "            total_dt += len(X_test)\n",
    "\n",
    "    print(f\"loss = {epoch_loss / len(train)}\", f\"test loss = {epoch_test_loss / len(test)} accuracy = {correct / total_dt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501c1564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5881, 0.4119]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "tokens = tokenizer([\n",
    "    \"wi = 1\"\n",
    "], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "logits = model(**tokens).logits\n",
    "torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.classifier.state_dict(), \"data/roberta_figure_diff.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615bf324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "# inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "\n",
    "# predicted_class_id = logits.argmax().item()\n",
    "# model.config.id2label[predicted_class_id]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"FacebookAI/roberta-base\", num_labels=2)\n",
    "model.classifier.load_state_dict(torch.load(\"data/roberta_figure_diff.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b6a42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3877, 0.6123]], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "tokens = tokenizer([\n",
    "    \"Financial Asset Loss per Annum\"\n",
    "], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "logits = model(**tokens).logits\n",
    "torch.softmax(logits, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
